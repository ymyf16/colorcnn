{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5de090dd-d604-4c01-a411-4e8734d450c2",
   "metadata": {},
   "source": [
    "## Color CNN implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232ca85a-b934-43df-934d-ec2ca493b821",
   "metadata": {},
   "source": [
    "if can find the kernel, use colorcnn kernel\n",
    "\n",
    "or do in the terminal:\n",
    "\n",
    "conda create -n yournamefortheenv python=3.10 #more recent versions don't work with pytorch\n",
    "\n",
    "conda install pytorch\n",
    "\n",
    "conda install torchvision\n",
    "\n",
    "conda install cuda -c nvidia\n",
    "\n",
    "#etc... Whatever's missing in the error message just install it; if conda install didn't work, try **pip install** or **pip3 install**\n",
    "\n",
    "**before running this notebook**, do in the terminal:\n",
    "\n",
    "module load cuda/11.8 #To start using gpu\n",
    "\n",
    "conda activate colorcnn #in the env\n",
    "\n",
    "then restart the kernel and reopen the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8c204c5-cbd3-4f6a-a6c1-6da525637644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# use gpu if available\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adbabc80-12c1-4e09-9174-5c83ad575d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import shutil\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.models import VGG16_Weights\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43d1a39-f5a8-4651-9e2c-fd178fb55aef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Dataset preprocessing\n",
    "(Don't need to run it if the val_processed is there already)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e81e77-fd15-4bbf-b871-e41b10324f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dirs\n",
    "data_root            = \"./tiny-imagenet-200-data\"\n",
    "val_images_dir       = os.path.join(data_root, \"val/images\")\n",
    "val_ann_file         = os.path.join(data_root, \"val/val_annotations.txt\")\n",
    "words_file           = os.path.join(data_root, \"words.txt\")\n",
    "processed_val_dir    = os.path.join(data_root, \"val_processed\")\n",
    "# paras\n",
    "batch_size = 32 #don't know whether we should use something different\n",
    "\n",
    "# 2. Read synset → human label map\n",
    "synset_to_label = {}\n",
    "with open(words_file, 'r') as f:\n",
    "    for line in f:\n",
    "        synset, label = line.strip().split('\\t')\n",
    "        synset_to_label[synset] = label\n",
    "\n",
    "# 3. Read filename → synset map\n",
    "filename_to_synset = {}\n",
    "with open(val_ann_file, 'r') as f:\n",
    "    for line in f:\n",
    "        fname, syn, *rest = line.strip().split('\\t')\n",
    "        filename_to_synset[fname] = syn\n",
    "\n",
    "# 4. Reorganize: move each image into val_processed/<synset>_<label>/\n",
    "for fname, syn in filename_to_synset.items():\n",
    "    src_path = os.path.join(val_images_dir, fname)\n",
    "    if not os.path.exists(src_path):\n",
    "        continue\n",
    "    # use the first comma-separated token of the human label, underscored\n",
    "    human = synset_to_label[syn].split(',')[0].replace(' ', '_')\n",
    "    class_folder = f\"{syn}_{human}\"\n",
    "    dst_dir = os.path.join(processed_val_dir, class_folder)\n",
    "    os.makedirs(dst_dir, exist_ok=True)\n",
    "    shutil.copy(src_path, os.path.join(dst_dir, fname))\n",
    "\n",
    "print(\"Reorganized val into:\", processed_val_dir)\n",
    "\n",
    "\n",
    "# 5. Define transforms using the original ImageNet mean/std \n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std  = [0.229, 0.224, 0.225] \n",
    "\n",
    "#I tried to manually calculate the mean & std for our subset \n",
    "#but then realized it doesn't make sense cuz the model is trained on the complete one; \n",
    "#so we just use the standard paras here\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n",
    "])\n",
    "\n",
    "# 6. Create data loaders\n",
    "val_dataset = datasets.ImageFolder(processed_val_dir, transform=val_transform)\n",
    "val_loader  = DataLoader(val_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False,\n",
    "                         num_workers=4,\n",
    "                         pin_memory=True)\n",
    "\n",
    "print(\"Number of classes in val:\", len(val_dataset.classes))\n",
    "print(\"Number of val images:\", len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24631a31-5881-49a7-9993-51bb1d65363a",
   "metadata": {},
   "source": [
    "## Load dataset to VGG16\n",
    "and create the vgg16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c3cd7ca-ad0d-423d-b74a-b5487696d7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dirs\n",
    "data_root            = \"./tiny-imagenet-200-data\"\n",
    "val_images_dir       = os.path.join(data_root, \"val/images\")\n",
    "val_ann_file         = os.path.join(data_root, \"val/val_annotations.txt\")\n",
    "words_file           = os.path.join(data_root, \"words.txt\")\n",
    "processed_val_dir    = os.path.join(data_root, \"val_processed\")\n",
    "# paras\n",
    "batch_size = 32 #don't know whether we should use something different\n",
    "\n",
    "\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std  = [0.229, 0.224, 0.225] \n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n",
    "])\n",
    "\n",
    "# Create data loaders\n",
    "val_dataset = datasets.ImageFolder(processed_val_dir, transform=val_transform)\n",
    "val_loader  = DataLoader(val_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False,\n",
    "                         num_workers=4,\n",
    "                         pin_memory=True)\n",
    "# Point to subset\n",
    "dataset = datasets.ImageFolder(\n",
    "    processed_val_dir,\n",
    "    transform=val_transform\n",
    ")\n",
    "loader = DataLoader(dataset,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=False,\n",
    "                    num_workers=4,\n",
    "                    pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1703f3e2-1e37-453f-8bc4-3383ac011555",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16(weights=VGG16_Weights.DEFAULT)\n",
    "model.eval()   # inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52292863-aff6-447d-8b49-0b1e174ee313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if model worked by just grabbing one batch from DataLoader\n",
    "images, labels = next(iter(val_loader))  # shape: [B, 3, 224, 224]\n",
    "\n",
    "# 3. Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)              # shape: [B, 1000]\n",
    "\n",
    "print(\"Batch input shape :\", images.shape)\n",
    "print(\"Output logits shape:\", outputs.shape)\n",
    "\n",
    "# 4. Decode top-5 predictions for the first image\n",
    "probs = torch.softmax(outputs, dim=1)\n",
    "top5_prob, top5_lbl = probs[0].topk(5)\n",
    "for prob, idx in zip(top5_prob, top5_lbl):\n",
    "    print(f\"{val_dataset.classes[idx]:>30s} : {prob.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98938d07-cc06-4ab1-9118-d7e71a04d573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Inference loop\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in dataloader:\n",
    "        images = images.to(device)\n",
    "        outputs = vgg(images)               # raw logits\n",
    "        _, preds = torch.max(outputs, dim=1)  # predicted class indices\n",
    "\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(labels.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94860dd4-eb78-40a9-9dad-4e166f5e7f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load VGG-16 with the default ImageNet-pretrained weights\n",
    "vgg = models.vgg16(weights=VGG16_Weights.DEFAULT)\n",
    "print(\"Successfully loaded vgg\")\n",
    "vgg.eval()\n",
    "vgg.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e5a643-acb0-481c-b0da-7edbef4112de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Compute simple accuracy\n",
    "correct = sum(p==t for p, t in zip(all_preds, all_labels))\n",
    "total   = len(all_labels)\n",
    "print(f\"Top‐1 Accuracy: {correct/total*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colorcnn",
   "language": "python",
   "name": "colorcnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
