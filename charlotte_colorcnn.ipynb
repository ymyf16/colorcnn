{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96585f37-b0fd-40c5-89a9-8b78bb4ffca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 63/63 [01:20<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved neuron grid to neuron_grid_original_conv1_1.png\n",
      "Computing CSI for first 100 neurons...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██                                          | 3/64 [00:03<01:20,  1.32s/it]/var/folders/_k/km6g95q94wg8jj9s33zh2prm0000gn/T/ipykernel_57834/440622482.py:255: RuntimeWarning: divide by zero encountered in divide\n",
      "  csi = np.mean(1 - np.clip(norm_gray / norm_rgb, 0, 1))\n",
      " 30%|████████████▊                              | 19/64 [00:25<01:01,  1.36s/it]"
     ]
    }
   ],
   "source": [
    "# load the pre-trained VGG16 and specify which layer we'll extract feature activations from\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets.folder import default_loader\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as TF\n",
    "from IPython.display import display, Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import gc\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load pretrained VGG16\n",
    "vgg16 = models.vgg16(pretrained=True).eval()\n",
    "\n",
    "# Select the layer from which we want to extract neuron activations (e.g., conv5_3)\n",
    "target_layer = 0  # conv1_1\n",
    "\n",
    "# Truncate model at selected layer\n",
    "vgg16_truncated = nn.Sequential(*list(vgg16.features.children())[:target_layer + 1])\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg16_truncated = vgg16_truncated.to(device)\n",
    "\n",
    "# transform tinyimagenet validation images so that they are in the correct shape and size for VGG16; the class allows the transformation to be applied to all 10,000 images\n",
    "\n",
    "# Define transform: resize to 224x224 (VGG16 input size), convert to tensor, normalize\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to VGG16 input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet mean\n",
    "                         std=[0.229, 0.224, 0.225])   # ImageNet std\n",
    "])\n",
    "\n",
    "# Custom Dataset to load images from a folder\n",
    "class TinyImageNetValDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_paths = sorted([\n",
    "            os.path.join(image_dir, fname)\n",
    "            for fname in os.listdir(image_dir)\n",
    "            if fname.endswith(\".JPEG\")\n",
    "        ])\n",
    "        self.transform = transform\n",
    "        self.loader = default_loader\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.loader(self.image_paths[idx])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.image_paths[idx]  # return path for identification\n",
    "\n",
    "# Set path to your TinyImageNet val/images folder\n",
    "image_dir = \"/Users/charlotteimbert/Documents/SP2025/NEUR189B/tiny-imagenet-200/tiny-tiny/images\"\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TinyImageNetValDataset(image_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# create a hook to extract activations from neurons in specific layer; hook captures the layer outputs\n",
    "# Load pretrained VGG16\n",
    "vgg16 = models.vgg16(pretrained=True).eval().to(device)\n",
    "\n",
    "# Choose layer name, e.g., last convolutional layer\n",
    "layer_name = 'features.0'\n",
    "activations = {}\n",
    "\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Register the hook\n",
    "layer = dict([*vgg16.named_modules()])[layer_name]\n",
    "layer.register_forward_hook(get_activation(layer_name))\n",
    "\n",
    "# pass val images through the model and get activations\n",
    "activation_list = []\n",
    "image_paths = []\n",
    "\n",
    "# Adjust batch size as needed\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, paths in tqdm(loader):\n",
    "        images = images.to(device)\n",
    "        _ = vgg16(images)  # forward pass\n",
    "        batch_acts = activations[layer_name].cpu()  # shape: [B, C, H, W]\n",
    "        \n",
    "        activation_list.extend(batch_acts)\n",
    "        image_paths.extend(paths)\n",
    "        \n",
    "def get_top_patches(activation_list, image_paths, dataset, top_k=100):\n",
    "    neuron_patches = {}  # dict: neuron_idx -> list of patches\n",
    "\n",
    "    for neuron_idx in range(activation_list[0].shape[0]):\n",
    "        activations = []\n",
    "        for i, act in enumerate(activation_list):\n",
    "            max_val = act[neuron_idx].max().item()\n",
    "            activations.append((max_val, i))\n",
    "\n",
    "        # Sort and get top_k\n",
    "        top_k_indices = sorted(activations, key=lambda x: x[0], reverse=True)[:top_k]\n",
    "\n",
    "        patches = []\n",
    "        for _, img_idx in top_k_indices:\n",
    "            img_path = image_paths[img_idx]\n",
    "            original_img = Image.open(img_path).convert('RGB')\n",
    "            original_img = transform(original_img)  # same transform as dataset\n",
    "            patch_size = act.shape[1:]  # (H, W) of feature map\n",
    "            scale_factor = dataset[0][0].shape[1] / patch_size[1]  # image / activation width\n",
    "\n",
    "            # Get max location (x, y)\n",
    "            act_map = activation_list[img_idx][neuron_idx]\n",
    "            y, x = np.unravel_index(torch.argmax(act_map).item(), act_map.shape)\n",
    "\n",
    "            # Convert to pixel coordinates\n",
    "            x1 = int(x * scale_factor)\n",
    "            y1 = int(y * scale_factor)\n",
    "            x2 = x1 + int(scale_factor)\n",
    "            y2 = y1 + int(scale_factor)\n",
    "\n",
    "            # Clip and crop\n",
    "            x1, y1, x2, y2 = map(lambda v: max(0, min(v, original_img.shape[1])), [x1, y1, x2, y2])\n",
    "            patch = TF.crop(original_img, y1, x1, y2 - y1, x2 - x1)\n",
    "            patches.append(patch)\n",
    "\n",
    "        neuron_patches[neuron_idx] = patches\n",
    "\n",
    "    return neuron_patches\n",
    "neuron_patches = get_top_patches(activation_list, image_paths, dataset)\n",
    "\n",
    "def compute_neuron_features(neuron_patches, activation_list, top_k=100):\n",
    "    neuron_features = {}\n",
    "\n",
    "    for neuron_idx, patches in neuron_patches.items():\n",
    "        if len(patches) == 0:\n",
    "            continue\n",
    "\n",
    "        # Convert patches to tensors and stack\n",
    "        patch_tensors = torch.stack([\n",
    "            p if isinstance(p, torch.Tensor) else ToTensor()(p)\n",
    "            for p in patches\n",
    "        ])\n",
    "\n",
    "        # Optionally use activations to weight patches\n",
    "        activations = []\n",
    "        for img_idx in range(top_k):\n",
    "            act_map = activation_list[img_idx][neuron_idx]\n",
    "            max_val = act_map.max().item()\n",
    "            activations.append(max_val)\n",
    "\n",
    "        activations = torch.tensor(activations)\n",
    "        norm_activations = activations / activations.sum()\n",
    "\n",
    "        # Compute weighted average (neuron feature)\n",
    "        nf = (patch_tensors * norm_activations[:, None, None, None]).sum(dim=0)  # shape: (3, H, W)\n",
    "        neuron_features[neuron_idx] = nf\n",
    "\n",
    "    return neuron_features\n",
    "\n",
    "neuron_features = compute_neuron_features(neuron_patches, activation_list)\n",
    "\n",
    "def save_neuron_features(neuron_features, out_dir=\"neuron_features\", max_to_save=None):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    for i, (neuron_idx, nf_tensor) in enumerate(neuron_features.items()):\n",
    "        if max_to_save is not None and i >= max_to_save:\n",
    "            break\n",
    "        nf_tensor = nf_tensor.cpu().clamp(0, 1)  # move to CPU and clip\n",
    "        img = ToPILImage()(nf_tensor)\n",
    "        img.save(os.path.join(out_dir, f\"neuron_{neuron_idx}.png\"))\n",
    "        del nf_tensor, img\n",
    "        gc.collect()\n",
    "\n",
    "save_neuron_features(neuron_features, max_to_save=100)\n",
    "\n",
    "image_dir = \"neuron_features\"\n",
    "\n",
    "def create_neuron_grid(neuron_features, grid_size=(10, 10), out_path=\"neuron_grid.png\"):\n",
    "    fig, axes = plt.subplots(*grid_size, figsize=(20, 20))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, (neuron_idx, nf_tensor) in enumerate(neuron_features.items()):\n",
    "        if i >= grid_size[0] * grid_size[1]:\n",
    "            break\n",
    "        nf_tensor = nf_tensor.cpu().clamp(0, 1)\n",
    "        img = ToPILImage()(nf_tensor)\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f\"Neuron {neuron_idx}\", fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path)\n",
    "    plt.close()\n",
    "    print(f\"Saved neuron grid to {out_path}\")\n",
    "create_neuron_grid(neuron_features, grid_size=(10, 10), out_path=\"neuron_grid_original_conv1_1.png\")\n",
    "\n",
    "# ---------- Helper Functions from Nefesi ----------\n",
    "\n",
    "def rgb2opp(img_np):\n",
    "    \"\"\"Convert RGB image to opponent color space (approximate).\"\"\"\n",
    "    R, G, B = img_np[..., 0], img_np[..., 1], img_np[..., 2]\n",
    "    O1 = (R - G) / np.sqrt(2)\n",
    "    O2 = (R + G - 2 * B) / np.sqrt(6)\n",
    "    O3 = (R + G + B) / np.sqrt(3)  # Intensity channel\n",
    "    return np.stack([O1, O2, O3], axis=-1)\n",
    "\n",
    "def image2max_gray(opp_img):\n",
    "    \"\"\"Collapse color channels to max projection of absolute value.\"\"\"\n",
    "    return np.max(np.abs(opp_img), axis=-1)\n",
    "\n",
    "# ---------- Main CSI Function ----------\n",
    "\n",
    "def compute_csi_for_neuron(neuron_idx, activation_list, image_paths, transform):\n",
    "    grayscale_activations = []\n",
    "    rgb_activations = []\n",
    "\n",
    "    for i, act in enumerate(activation_list):\n",
    "        act_map = act[neuron_idx]  # shape: [H, W]\n",
    "        max_val = act_map.max().item()\n",
    "        rgb_activations.append(max_val)\n",
    "\n",
    "        # Load image and convert to numpy\n",
    "        img = Image.open(image_paths[i]).convert('RGB')\n",
    "        img_resized = img.resize((224, 224))\n",
    "        img_np = np.array(img_resized).astype(np.float32) / 255.0\n",
    "\n",
    "        # Convert to grayscale using opponent space\n",
    "        im_opp = rgb2opp(img_np)\n",
    "        im_gray = image2max_gray(im_opp)\n",
    "\n",
    "        # Simulated grayscale activation: just max value in gray image\n",
    "        gray_max_val = np.max(im_gray)\n",
    "        grayscale_activations.append(gray_max_val)\n",
    "\n",
    "    rgb_activations = np.array(rgb_activations)\n",
    "    gray_activations = np.array(grayscale_activations)\n",
    "\n",
    "    if np.sum(rgb_activations) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    norm_rgb = rgb_activations / np.max(rgb_activations)\n",
    "    norm_gray = gray_activations / np.max(rgb_activations)\n",
    "\n",
    "    csi = np.mean(1 - np.clip(norm_gray / norm_rgb, 0, 1))\n",
    "    return csi\n",
    "\n",
    "csi_values = []\n",
    "num_neurons = activation_list[0].shape[0]\n",
    "\n",
    "print(\"Computing CSI for first 100 neurons...\")\n",
    "for neuron_idx in tqdm(range(min(100, num_neurons))):\n",
    "    csi = compute_csi_for_neuron(neuron_idx, activation_list, image_paths, transform)\n",
    "    csi_values.append((neuron_idx, csi))\n",
    "\n",
    "\n",
    "df_csi = pd.DataFrame(csi_values, columns=[\"neuron_idx\", \"CSI\"])\n",
    "df_csi.to_csv(\"color_selectivity_indices.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879e8661-ce0a-4028-98bb-cd105881cf0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (finalproject)",
   "language": "python",
   "name": "finalproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
